import threading
import uuid
import time
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import Response
import tempfile
import shutil
from pathlib import Path
from paddleocr import PaddleOCRVL
from pypdf import PdfReader, PdfWriter # NEW: For splitting pages
import os

app = FastAPI(title="PaddleOCR Document Processing API")

JOBS = {}

def process_pdf_background(job_id: str, temp_dir_obj, input_file: Path, original_filename: str):
    try:
        # 1. Initialize Status
        print(f"Job {job_id}: Initializing pipeline...", flush=True)
        JOBS[job_id]["status"] = "initializing"
        JOBS[job_id]["progress"] = "Loading AI Model..."
        
        pipeline = PaddleOCRVL(
            vl_rec_backend="vllm-server", 
            vl_rec_server_url="http://136.107.44.152:8000/v1"
        )
        
        # 2. Split PDF to count pages
        reader = PdfReader(str(input_file))
        total_pages = len(reader.pages)
        print(f"Job {job_id}: Found {total_pages} pages.", flush=True)
        
        all_results = []
        
        # 3. Process Page by Page
        for i in range(total_pages):
            current_page_num = i + 1
            
            # Update Job Status so CURL can see it
            JOBS[job_id]["status"] = "processing"
            JOBS[job_id]["progress"] = f"Processing page {current_page_num}/{total_pages}"
            print(f"Job {job_id}: Processing page {current_page_num}/{total_pages}...", flush=True)
            
            # Extract single page to temp file
            writer = PdfWriter()
            writer.add_page(reader.pages[i])
            
            single_page_path = input_file.parent / f"page_{i}.pdf"
            with open(single_page_path, "wb") as f_page:
                writer.write(f_page)
            
            # Predict JUST this page
            # Note: predict returns a list, so we take [0] or extend
            page_output = pipeline.predict(input=str(single_page_path))
            all_results.extend(page_output)
            
            # Clean up small temp file
            single_page_path.unlink()

        # 4. Final Compilation
        JOBS[job_id]["progress"] = "Compiling results..."
        
        markdown_list = []
        for res in all_results:
            markdown_list.append(res.markdown)
        
        markdown_texts = pipeline.concatenate_markdown_pages(markdown_list)
        
        JOBS[job_id]["result"] = markdown_texts
        JOBS[job_id]["status"] = "completed"
        JOBS[job_id]["progress"] = "Done"
        JOBS[job_id]["filename"] = f"{Path(original_filename).stem}.md"
        
        temp_dir_obj.cleanup()
        print(f"Job {job_id}: Finished successfully", flush=True)

    except Exception as e:
        JOBS[job_id]["status"] = "failed"
        JOBS[job_id]["error"] = str(e)
        print(f"Job {job_id}: FAILED - {str(e)}", flush=True)
        try:
            temp_dir_obj.cleanup()
        except:
            pass

@app.post("/submit-document")
async def submit_document(file: UploadFile = File(...)):
    job_id = str(uuid.uuid4())
    temp_dir = tempfile.TemporaryDirectory()
    temp_path = Path(temp_dir.name)
    input_file = temp_path / file.filename
    
    with open(input_file, "wb") as f:
        shutil.copyfileobj(file.file, f)

    JOBS[job_id] = {
        "status": "queued",
        "progress": "Waiting to start...",
        "submitted_at": time.time(),
        "original_filename": file.filename
    }

    thread = threading.Thread(
        target=process_pdf_background, 
        args=(job_id, temp_dir, input_file, file.filename)
    )
    thread.start()

    return {"job_id": job_id, "status": "queued"}

@app.get("/status/{job_id}")
async def check_status(job_id: str):
    if job_id not in JOBS:
        raise HTTPException(status_code=404, detail="Job ID not found")
    
    job = JOBS[job_id]
    
    # Return the new 'progress' field
    return {
        "job_id": job_id, 
        "status": job["status"],
        "progress": job.get("progress", "Unknown"), 
        "error": job.get("error"),
        "download_url": f"/download/{job_id}" if job["status"] == "completed" else None
    }

@app.get("/download/{job_id}")
async def download_result(job_id: str):
    if job_id not in JOBS:
        raise HTTPException(status_code=404, detail="Job ID not found")
    if JOBS[job_id]["status"] != "completed":
        raise HTTPException(status_code=400, detail="Job not finished yet")

    return Response(
        content=JOBS[job_id]["result"],
        media_type="text/markdown",
        headers={"Content-Disposition": f"attachment; filename={JOBS[job_id]['filename']}"}
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)